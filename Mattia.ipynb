{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ba3847-a73c-4ad2-9635-95e87a04abd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework 3 - Places of the world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ca2b4-9abf-4ce9-8c21-38cd2080cf60",
   "metadata": {},
   "source": [
    "## Libriaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a2ad83-0386-4aff-8ebc-52f83d38edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "import os\n",
    "from time import perf_counter\n",
    "from clear_cache import clear as clean_cache\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26335054-8e94-4ef7-bd76-6c593257bcc0",
   "metadata": {},
   "source": [
    "### 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4bf5a1-f1fd-4d28-a47c-8f9ef646eea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1. Get the list of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ffe2b-c30f-41e7-9a75-67a160282e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"urls.txt\", \"w\")\n",
    "\n",
    "for i in range(1,401):\n",
    "    url = f'https://www.atlasobscura.com/places?page={i}&sort=likes_count'\n",
    "    reqs = requests.get(url)\n",
    "    soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "    for link in soup.find_all(class_=re.compile(\"content-card content-card-place\")):\n",
    "        a=(link.get('href'))\n",
    "        f.write('https://www.atlasobscura.com/'+a)\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16452c4-5736-48c6-9bef-c756e31c01e6",
   "metadata": {},
   "source": [
    "#### 1.2. Crawl places\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd88715-6919-4714-8015-e8d1bdf80631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download(a):\n",
    "    times_={}\n",
    "    for i in range(1,401):\n",
    "        url = f'https://www.atlasobscura.com/places?page={i}&sort=likes_count'\n",
    "       perf_counterrt_time = perf_counter()\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        os.makedirs('PAGE '+str(i))\n",
    "        os.chdir('PAGE '+str(i))\n",
    "        count=i\n",
    "        for link in soup.find_all(class_=re.compile(\"content-card content-card-place\")):\n",
    "            a=(link.get('href'))\n",
    "            url_='https://www.atlasobscura.com/'+a\n",
    "            file= open(str(a)[8:]+'.html','w',encoding=\"UTF-8\")\n",
    "            with urlopen( url_ ) as webpage:\n",
    "                content = webpage.read().decode()\n",
    "            file.write(content)\n",
    "            file.close()\n",
    "        end_time = perf_counter()\n",
    "        final_time = end_time - start_time\n",
    "        if count not in times:\n",
    "            times_[count]=final_time\n",
    "        os.chdir('..')\n",
    "        clean_cache()\n",
    "\n",
    "N = multiprocessing.cpu_count()*2\n",
    "with multiprocessing.Pool(processes=N) as pool:\n",
    "     pool.map(download('urls.txt'),range(100))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e540a306-b1fc-489c-ae6d-1cb291f3c374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\matti\\\\OneDrive\\\\Desktop\\\\ADM\\\\REPO\\\\Homework-3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6a1c1-5aba-49b8-a7cb-851865ae4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "times={}\n",
    "for i in range(1,101):\n",
    "        url = f'https://www.atlasobscura.com/places?page={i}&sort=likes_count'\n",
    "        start_time = perf_counter()\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        os.makedirs('PAGE '+str(i))\n",
    "        os.chdir('PAGE '+str(i))\n",
    "        count=i\n",
    "        for link in soup.find_all(class_=re.compile(\"content-card content-card-place\")):\n",
    "            a=(link.get('href'))\n",
    "            url_='https://www.atlasobscura.com/'+a\n",
    "            file= open(str(a)[8:]+'.html','w',encoding=\"UTF-8\")\n",
    "            with urlopen( url_ ) as webpage:\n",
    "                content = webpage.read().decode()\n",
    "            file.write(content)\n",
    "            file.close()\n",
    "        end_time = perf_counter()\n",
    "        final_time = end_time - start_time\n",
    "        if count not in times:\n",
    "            times[count]=final_time\n",
    "        os.chdir('..')\n",
    "        clean_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57191a38-3263-41b8-8c87-0a473cf681ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 47.217420000000004, 2: 81.355642}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5625e-d988-44a3-af32-0a617d503de5",
   "metadata": {},
   "source": [
    "#### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679594bd-4261-4f4b-9ef5-c4a937a38a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,401):\n",
    "    os.chdir('PAGE '+str(i))\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        if filename.endswith(\".html\"):\n",
    "            with open(os.path.join(os.getcwd(), filename), 'r',encoding='utf-8') as f:\n",
    "                soup = BeautifulSoup(f,'html.parser')\n",
    "                out=[]\n",
    "                tag=[]\n",
    "\n",
    "\n",
    "\n",
    "                [out.append(i.text) for i in soup.find_all(class_=re.compile(\"DDPage__header-title\"))]\n",
    "                [tag.append(i.text[1:-1]) for i in soup.find_all(class_=re.compile(\"itemTags__link js-item-tags-link\"))]\n",
    "                out.append(tag)\n",
    "                a=[int(i.text) for i in soup.find_all(class_=re.compile(\"title-md item-action-count\"))]\n",
    "                out.append(a[0])\n",
    "                out.append(a[1])\n",
    "                [out.append(i.text.replace('\\n',' ')[2:-2]) for i in soup.find_all(class_=re.compile(\"DDP__body-copy\"))]\n",
    "                [out.append(i.text[:-1]) for i in soup.find_all(class_=re.compile(\"DDPage__header-dek\"))]\n",
    "                b=[(i.text.replace('\\n',' ')[1:-1]) for i in soup.find_all(class_=re.compile(\"DDPageSiderailRecirc__item-text\"))]\n",
    "                out.append(b)\n",
    "                [out.append(i.text.replace('\\n','')) for i in soup.select('#place-container > div.DDPage__content-row.grid-row > div.DDPageSiderail__column.grid-col-lg-4.grid-col-md-5 > div.DDPageSiderail > aside.DDPageSiderail__details > address > div')]\n",
    "                c=[(i.text.replace('\\n','').split(',')) for i in soup.select('#place-container > div.DDPage__content-row.grid-row > div.DDPageSiderail__column.grid-col-lg-4.grid-col-md-5 > div.DDPageSiderail > aside.DDPageSiderail__details > div')]\n",
    "                out.append(float(c[0][0]))\n",
    "                out.append(float(c[0][1]))\n",
    "                cont=[]\n",
    "                [cont.append(i.text) for i in soup.select('#ugc-module > div > div:nth-child(2) > div.DDPContributorsList > a')]\n",
    "                [cont.append(i.string) for i in soup.select('li>a>span') if i.string!=None]\n",
    "                out.append(cont) \n",
    "                for i in soup.find_all(class_=re.compile('DDPContributor__name')):\n",
    "                    time=(i.text.replace(',',''))\n",
    "                if isinstance(a,str)==True:\n",
    "                    time=(datetime.strptime(time,'%B %d %Y').date())\n",
    "                    out.append(time)\n",
    "                else:\n",
    "                    out.append(time)\n",
    "                lists=[]\n",
    "                [lists.append(i.text) for i in (soup.select('#page-content > article > div.vue-js-bte-place-parent.hidden-print.js-vue-component-wrap > div:nth-child(6) > div > div > div.card-grid.CardRecircSection__card-grid.js-inject-gtm-data-in-child-links>div>div>a>div>h3>span'))]\n",
    "                out.append(lists)\n",
    "                rel=[]\n",
    "                [rel.append(i.text) for i in (soup.select('#page-content > article > div.vue-js-bte-place-parent.hidden-print.js-vue-component-wrap > div:nth-child(4) > div > div > div.card-grid.CardRecircSection__card-grid.js-inject-gtm-data-in-child-links>div>div>a>div>h3>span'))]\n",
    "                out.append(rel)\n",
    "                out.append(soup.find('link',{'rel':'canonical'})['href'])\n",
    "\n",
    "                f.close()\n",
    "                l = ['placeName','placeTags','numPeopleVisited','numPeopleWant','placeDesc','placeShortDesc','placeNearby','placeAddress','placeAlt',\n",
    "                    'placeLong','placeEditors','placePubDate','placeRelatedLists','placeRelatedPlaces','placeURL']\n",
    "                with open(filename+'.tsv','w',encoding='utf-8') as tsv:\n",
    "                    tsv_output = csv.writer(tsv, delimiter='\\t')\n",
    "                    tsv_output.writerow(l)\n",
    "                    tsv_output.writerow(out)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650373d-f92f-4a15-834a-c3d4a9be9fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for i in range(1,401):\n",
    "    os.chdir('PAGE '+str(i))\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        if filename.endswith(\".tsv\"):\n",
    "            a = pd.read_csv(filename,sep='\\t')\n",
    "            data.append(a)\n",
    "    os.chdir('..')\n",
    "data=pd.concat(data,ignore_index=True)     \n",
    "data.to_csv('Dataset.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139ef58a-e955-4711-8ed3-5c93f33ab1aa",
   "metadata": {},
   "source": [
    "Cosin Similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c2e3ce-8d1e-45ac-86a1-6d0e63d62b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_CosSimi(K):\n",
    "    #dataset=pd.read_csv('Dataset.tsv',sep='\\t')\n",
    "    with open('dictionary_index.txt','r') as d:\n",
    "        dictionary = d.read()\n",
    "        cont_docs = json.loads(dictionary)\n",
    "    \n",
    "    query=str(input())\n",
    "    \n",
    "    a=[query]\n",
    "    b=query.split()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    query_tfidf = vectorizer.fit_transform(b)\n",
    "    \n",
    "    l=[]\n",
    "    for i in b:\n",
    "        l.append(word_dict[(i)])\n",
    "    doc_query=[]\n",
    "    for i in l:\n",
    "        doc_query.append(cont_docs[str(i)])\n",
    "    \n",
    "\n",
    "    common_doc=list(set.intersection(*map(set,doc_query)))\n",
    "    if common_doc==[]:\n",
    "        print('The query is not present in any document ')\n",
    "    else:\n",
    "        \n",
    "    \n",
    "        result = vectorizer.fit_transform(dataset['Processed_strings'].iloc[common_doc])\n",
    "        array_tfidf = result.toarray()\n",
    "\n",
    "        tfidf_df = pd.DataFrame(array_tfidf, index = dataset['placeName'].iloc[common_doc], columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "        cosineSimilarities = cosine_similarity(query_tfidf, tfidf_df[b])\n",
    "\n",
    "        dfcs=pd.DataFrame(cosineSimilarities,index=b,columns=dataset['placeName'].iloc[common_doc])\n",
    "        similarity=list(np.array(dfcs)[0])\n",
    "        x=dataset.iloc[common_doc, [0, 4, -3]]\n",
    "        x['similarity']=similarity\n",
    "        x=x.sort_values(by='similarity',ascending=False).head(K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fc2e15-6335-4d82-ba75-9cf6c4402705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_CosSim_2(K):\n",
    "    with open('dictionary_index.txt','r') as d:\n",
    "        dictionary = d.read()\n",
    "        cont_docs = json.loads(dictionary)\n",
    "    \n",
    "    query=str(input())\n",
    "    \n",
    "    a=[query]\n",
    "    b=query.split()\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    query_tfidf = vectorizer.fit_transform(a)\n",
    "    \n",
    "    l=[]\n",
    "    for i in b:\n",
    "        l.append(word_dict[(i)])\n",
    "    doc_query=[]\n",
    "    for i in l:\n",
    "        doc_query.append(cont_docs[str(i)])\n",
    "    doc_query1=[]\n",
    "    for i in doc_query:\n",
    "        for j in i:\n",
    "            doc_query1.append(j)\n",
    "    doc_query1=list(set(doc_query1))\n",
    "    \n",
    "    \n",
    "    if doc_query1==[]:\n",
    "        print('The query is not present in any document ')\n",
    "    else:\n",
    "        \n",
    "    \n",
    "        result = vectorizer.fit_transform(dataset['Processed_strings'].iloc[doc_query1])\n",
    "        array_tfidf = result.toarray()\n",
    "\n",
    "        tfidf_df = pd.DataFrame(array_tfidf, index = dataset['placeName'].iloc[doc_query1], columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "        cosineSimilarities = cosine_similarity(query_tfidf, tfidf_df[b])\n",
    "\n",
    "        dfcs=pd.DataFrame(cosineSimilarities,index=a,columns=dataset['placeName'].iloc[doc_query1])\n",
    "        similarity=list(np.array(dfcs)[0])\n",
    "        x=dataset.iloc[doc_query1, [0, 4, -3]]\n",
    "        x['similarity']=similarity\n",
    "        x=x.sort_values(by='similarity',ascending=False).head(K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a38c3-8111-498a-9a00-7ffb0c22c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Insert your address in the following format : street, house number, town')\n",
    "address=str(input())\n",
    "def query_3(address):\n",
    "    \n",
    "    \n",
    "    url = 'https://nominatim.openstreetmap.org/search/' + urllib.parse.quote(address) +'?format=json'\n",
    "\n",
    "    response = requests.get(url).json()\n",
    "    lat=(response[0]['lat'])\n",
    "    lon=(response[0]['lon'])\n",
    "    loc=(float(lat),float(lon))\n",
    "    df =pd.read_csv('dataset.tsv',sep='\\t')\n",
    "    \n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "address = 'Viale Ippocrate, 10'\n",
    "url = 'https://nominatim.openstreetmap.org/search/' + urllib.parse.quote(address) +'?format=json'\n",
    "\n",
    "response = requests.get(url).json()\n",
    "lat=(response[0]['lat'])\n",
    "lon=(response[0]['lon'])\n",
    "loc=(float(lat),float(lon))\n",
    "loc\n",
    "\n",
    "import haversine as hv\n",
    "loc1=(28.426846,77.088834)\n",
    "loc2=(28.394231,77.050308)\n",
    "round(hv.haversine(loc,loc2),2)\n",
    "\n",
    "adr=(41.9091945, 12.520531)\n",
    "a=[1,67,900,87]\n",
    "b=dataset.iloc[a]\n",
    "locs=dataset.iloc[a,[8,9]].values\n",
    "lista=[]\n",
    "for i in locs:\n",
    "    lista.append(tuple(i))\n",
    "b['Loc']=lista\n",
    "dist=[]\n",
    "for i in b.Loc.values:\n",
    "    dist.append(round(hv.haversine(adr,i),2))\n",
    "b['distance']=dist\n",
    "pop=dataset.iloc[a,[2,3]].values\n",
    "b['popularity']=(0.7*dataset.iloc[a,2]+0.3*dataset.iloc[a,3])\n",
    "b['popularity']=abs(0.05-(b['popularity'] - np.min(b['popularity']))/(np.max(b['popularity']-np.min(b['popularity']))))\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
